{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from scipy.fft import fft\n","from scipy.stats import kurtosis, skew\n","from scipy.ndimage import gaussian_filter1d\n","\n","\n","def preprocess_data(file_path, appliance_type):\n","    data = pd.read_csv(file_path)\n","    data['time'] = pd.to_datetime(data['time']).dt.tz_localize(None)\n","    data.set_index('time', inplace=True)\n","    data['kWh.mean_value'] = data['kWh.mean_value'].interpolate().ffill().bfill()\n","\n","    data['day_of_week'] = data.index.dayofweek\n","    data['hour'] = data.index.hour\n","    data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)\n","    data['month'] = data.index.month\n","    data['season'] = (data['month'] % 12 + 3) // 3\n","\n","    energy_scaler = MinMaxScaler()\n","    data['kWh.mean_value'] = energy_scaler.fit_transform(data[['kWh.mean_value']]).astype(np.float32)\n","    \n","    time_scaler = MinMaxScaler()\n","    time_features = ['day_of_week', 'hour', 'is_weekend', 'month', 'season']\n","    data[time_features] = time_scaler.fit_transform(data[time_features]).astype(np.float32)\n","    \n","    return {\n","        'data': data,\n","        'energy_scaler': energy_scaler,\n","        'time_scaler': time_scaler\n","    }\n","\n","\n","\n","\n","np.random.seed(48)\n","tf.random.set_seed(48)\n","\n","\n","\n","def prepare_sequences(data, seq_len=6):\n","    X = []\n","    for i in range(len(data) - seq_len + 1):\n","        X.append(data.iloc[i:i+seq_len].values)\n","    return np.array(X, dtype=np.float32)\n","\n","class MultiScaleAttention(tf.keras.layers.Layer):\n","    def __init__(self, hidden_dim):\n","        super(MultiScaleAttention, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=hidden_dim)\n","        \n","    def call(self, inputs):\n","        attention_output = self.attention(inputs, inputs)\n","        return inputs + attention_output\n","\n","class TimeGAN(tf.keras.Model):\n","    def __init__(self, seq_len, n_features, hidden_dim, appliance_type):\n","        super(TimeGAN, self).__init__()\n","        self.seq_len = seq_len\n","        self.n_features = n_features\n","        self.hidden_dim = hidden_dim\n","        self.appliance_type = appliance_type\n","        \n","        self.generator = self.build_generator()\n","        self.discriminator = self.build_discriminator()\n","        self.embedder = self.build_embedder()\n","        self.recovery = self.build_recovery()\n","        self.peak_generator = self.build_peak_generator()\n","\n","        self.sparsity_weight = tf.Variable(1.0, trainable=False)\n","    \n","    def build_generator(self):\n","        inputs = tf.keras.Input(shape=(self.seq_len, self.n_features))\n","        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))(inputs)\n","        x = MultiScaleAttention(self.hidden_dim)(x)\n","        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))(x)\n","        x = MultiScaleAttention(self.hidden_dim)(x)\n","        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))(x)\n","        outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.n_features))(x)\n","        return tf.keras.Model(inputs=inputs, outputs=outputs)\n","    \n","    def build_discriminator(self):\n","        inputs = tf.keras.Input(shape=(self.seq_len, self.n_features))\n","        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))(inputs)\n","        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim))(x)\n","        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","        return tf.keras.Model(inputs=inputs, outputs=outputs)\n","    \n","    def build_embedder(self):\n","        return tf.keras.Sequential([\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True, input_shape=(self.seq_len, self.n_features))),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=False)),\n","            tf.keras.layers.Dense(self.hidden_dim)\n","        ])\n","    \n","    def build_recovery(self):\n","        return tf.keras.Sequential([\n","            tf.keras.layers.RepeatVector(self.seq_len),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True)),\n","            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.n_features))\n","        ])\n","\n","    def build_peak_generator(self):\n","        inputs = tf.keras.Input(shape=(self.seq_len, self.n_features))\n","        x = tf.keras.layers.Dense(self.hidden_dim, activation='relu')(inputs)\n","        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","        return tf.keras.Model(inputs=inputs, outputs=x)\n","\n","    @tf.function\n","    def train_step(self, real_data):\n","        batch_size = tf.shape(real_data)[0]\n","        random_noise = tf.random.normal([batch_size, self.seq_len, self.n_features], dtype=tf.float32)\n","        \n","        with tf.GradientTape(persistent=True) as tape:\n","            fake_data = self.generator(random_noise, training=True)\n","            \n","            real_output = self.discriminator(real_data, training=True)\n","            fake_output = self.discriminator(fake_data, training=True)\n","            \n","            real_embedded = self.embedder(real_data, training=True)\n","            fake_embedded = self.embedder(fake_data, training=True)\n","            \n","            recovered_data = self.recovery(real_embedded, training=True)\n","            \n","            peak_real = self.peak_generator(real_data, training=True)\n","            peak_fake = self.peak_generator(fake_data, training=True)\n","            \n","            g_loss = -tf.reduce_mean(tf.math.log(fake_output + 1e-8))\n","            d_loss = -tf.reduce_mean(tf.math.log(real_output + 1e-8) + tf.math.log(1. - fake_output + 1e-8))\n","            e_loss = tf.reduce_mean(tf.square(real_embedded - fake_embedded))\n","            r_loss = tf.reduce_mean(tf.abs(recovered_data - real_data))\n","            \n","            daily_pattern_loss = tf.reduce_mean(tf.abs(tf.reduce_mean(fake_data[:, :, 0], axis=0) - tf.reduce_mean(real_data[:, :, 0], axis=0)))\n","            weekly_pattern_loss = tf.reduce_mean(tf.abs(tf.reduce_mean(fake_data[:, :, 1], axis=1) - tf.reduce_mean(real_data[:, :, 1], axis=1)))\n","            variability_loss = tf.abs(tf.math.reduce_std(fake_data[:, :, 0]) - tf.math.reduce_std(real_data[:, :, 0]))\n","            scale_loss = tf.abs(tf.reduce_max(fake_data[:, :, 0]) - tf.reduce_max(real_data[:, :, 0]))\n","            \n","            pattern_loss = daily_pattern_loss + weekly_pattern_loss + variability_loss + scale_loss\n","            \n","            peak_loss = tf.reduce_mean(tf.abs(peak_real - peak_fake))\n","            \n","            if self.appliance_type == 'PC':\n","                sparsity_threshold = 0.2\n","                sparsity_loss = tf.reduce_mean(tf.maximum(fake_data[:, :, 0] - sparsity_threshold, 0))\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 30 * pattern_loss + self.sparsity_weight * 80 * sparsity_loss + 75 * peak_loss\n","            \n","            elif self.appliance_type == 'water':\n","                sparsity_threshold = 0.05\n","                sparsity_loss = tf.reduce_mean(tf.maximum(fake_data[:, :, 0] - sparsity_threshold, 0))\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 45 * pattern_loss + self.sparsity_weight * sparsity_loss + 30 * peak_loss\n","            \n","            elif self.appliance_type == 'microwave':\n","                smoothness_loss = tf.reduce_mean(tf.abs(fake_data[:, 1:, 0] - fake_data[:, :-1, 0]))\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 110 * pattern_loss + 70 * smoothness_loss + 35 * peak_loss\n","\n","            elif self.appliance_type == 'coffee':\n","                weekday_mask = tf.cast(tf.less(real_data[:, :, 1], 4), tf.float32)  # Monday to Thursday\n","                weekend_mask = 1 - weekday_mask  # Friday to Sunday\n","\n","                hour = real_data[:, :, 2] * 24\n","                morning_peak_mask = tf.cast(tf.logical_and(tf.greater_equal(hour, 3), tf.less(hour, 7)), tf.float32)\n","                afternoon_peak_mask = tf.cast(tf.logical_and(tf.greater_equal(hour, 13), tf.less(hour, 15)), tf.float32)\n","\n","                weekday_loss = tf.reduce_mean(tf.abs(0.357 - fake_data[:, :, 0] * weekday_mask))\n","                weekend_loss = tf.reduce_mean(tf.abs(0.032 - fake_data[:, :, 0] * weekend_mask))\n","                peak_loss = tf.reduce_mean(tf.maximum(0.5 - fake_data[:, :, 0] * (morning_peak_mask + afternoon_peak_mask), 0))\n","\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 30 * pattern_loss + \\\n","                         100 * weekday_loss + 200 * weekend_loss + 150 * peak_loss\n","                \n","                \n","                \n","            elif self.appliance_type == 'printer':\n","                peak_loss = tf.reduce_mean(tf.abs(tf.reduce_max(fake_data[:, :, 0], axis=1) - tf.reduce_max(real_data[:, :, 0], axis=1)))\n","                variability_loss = tf.abs(tf.math.reduce_std(fake_data[:, :, 0]) - tf.math.reduce_std(real_data[:, :, 0]))\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 30 * pattern_loss + 50 * peak_loss + 20 * variability_loss\n","            else:\n","                smoothness_loss = tf.reduce_mean(tf.abs(fake_data[:, 1:, 0] - fake_data[:, :-1, 0]))\n","                total_loss = g_loss + d_loss + e_loss + r_loss + 110 * pattern_loss + 15 * smoothness_loss + 50 * peak_loss \n","        \n","        variables = self.generator.trainable_variables + self.discriminator.trainable_variables + \\\n","                    self.embedder.trainable_variables + self.recovery.trainable_variables + \\\n","                    self.peak_generator.trainable_variables\n","        \n","        gradients = tape.gradient(total_loss, variables)\n","        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n","        self.optimizer.apply_gradients(zip(gradients, variables))\n","        \n","        if self.appliance_type == 'PC':\n","            if sparsity_loss > 0.1:\n","                self.sparsity_weight.assign(tf.minimum(self.sparsity_weight * 5, 10.0))\n","            else:\n","                self.sparsity_weight.assign(tf.maximum(self.sparsity_weight * 0.9, 0.1))\n","        \n","        return g_loss, d_loss, e_loss, r_loss, pattern_loss, peak_loss\n","\n","def post_process_appliance(synthetic_df, appliance_type, energy_scaler, original_data):\n","    synthetic_df['kWh.mean_value'] = np.maximum(synthetic_df['kWh.mean_value'], 0)\n","    \n","    if appliance_type == 'printer':\n","        synthetic_df['kWh.mean_value'] = custom_printer_post_process(synthetic_df['kWh.mean_value'].values)\n","    elif appliance_type == 'water':\n","        synthetic_df['kWh.mean_value'] = custom_water_post_process(synthetic_df['kWh.mean_value'].values)\n","        \n","    elif appliance_type == 'coffee':\n","        synthetic_df['kWh.mean_value'] = custom_coffee_post_process(synthetic_df['kWh.mean_value'].values, synthetic_df.index)\n","  \n","    elif appliance_type =='microwave':\n","        synthetic_df['kWh.mean_value'] = custom_microwave_post_process(synthetic_df['kWh.mean_value'].values)\n","    return synthetic_df\n","\n","\n","\n","\n","\n","\n","\n","def custom_microwave_post_process(data):\n","    # Ensure data is in float32 format\n","    data = data.astype(np.float32)\n","    \n","    # Apply low usage pattern\n","    threshold = np.percentile(data, 10)\n","    low_usage_mask = data < threshold\n","    data[low_usage_mask] *= np.random.beta(0.6, 0.95, size=low_usage_mask.sum()).astype(np.float32)\n","    \n","    # Apply high usage pattern\n","    peak_threshold = np.percentile(data, 95)\n","    peak_mask = data > peak_threshold\n","    data[peak_mask] *= 0.95\n","    data[~peak_mask] *= 0.45\n","    \n","    # Add occasional spikes\n","    spike_prob = 0.07\n","    spike_mask = np.random.random(data.shape) < spike_prob\n","    data[spike_mask] = np.random.uniform(0.65, 0.95, size=spike_mask.sum()).astype(np.float32)\n","    \n","    # Ensure all values are within [0, 1] range\n","    data = np.clip(data, 0, 1)\n","    \n","    return data\n","\n","\n","\n","\n","def custom_water_post_process(data):\n","    # Apply a transformation to ground more often\n","    data = np.clip(data, a_min=0, a_max=None)  # Ensure data is non-negative\n","    data = data * np.random.uniform(0.6, 1.2, size=data.shape)  # Lower the overall baseline\n","    return data\n","\n","def custom_printer_post_process(data):\n","    \n","    baseline = np.min(data)\n","    data = np.full_like(data, baseline)\n","    \n","    # frequent small spikes\n","    small_spike_prob = 0.05\n","    small_spike_mask = np.random.random(data.shape) < small_spike_prob\n","    data[small_spike_mask] = np.random.uniform(0.01, 0.05, size=small_spike_mask.sum())\n","    \n","    #  occasional larger spikes\n","    large_spike_prob = 0.25\n","    large_spike_mask = np.random.random(data.shape) < large_spike_prob\n","    data[large_spike_mask] = np.random.uniform(0.5, 1.05, size=large_spike_mask.sum())\n","    \n","    # noise in non-spike periods\n","    non_spike_mask = ~(small_spike_mask | large_spike_mask)\n","    data[non_spike_mask] += np.random.uniform(0.15, 0.25, size=non_spike_mask.sum())\n","    \n","    # Smooth the data \n","    data = gaussian_filter1d(data, sigma=0.6)\n","    \n","    return data\n","\n","\n","\n","def custom_coffee_post_process(data, dates):\n","     # Create masks for weekdays and weekends\n","    weekday_mask = (dates.dayofweek < 4)  # Monday to Thursday\n","    weekend_mask = ~weekday_mask  # Friday to Sunday\n","\n","    # Create hour of day\n","    hour = dates.hour + dates.minute / 60\n","\n","    # Weekday pattern\n","    data[weekday_mask] = np.random.normal(0.357, 0.1, size=weekday_mask.sum())\n","    \n","    # Weekend pattern\n","    data[weekend_mask] = np.random.normal(0.0, 0.01, size=weekend_mask.sum())\n","\n","    # Morning peak (3 AM to 7 AM)\n","    morning_peak = (hour >= 3) & (hour < 7)\n","    data[morning_peak] *= np.random.uniform(1.2, 2.2, size=morning_peak.sum())\n","\n","    # Afternoon peak (1 PM to 3 PM)\n","    afternoon_peak = (hour >= 13) & (hour < 15)\n","    data[afternoon_peak] *= np.random.uniform(1.2, 2.2, size=afternoon_peak.sum())\n","\n","    # Early morning and late evening lows\n","    low_usage = ((hour >= 4) & (hour < 6)) | ((hour >= 17) & (hour < 19))\n","    data[low_usage] *= np.random.uniform(0.5, 0.8, size=low_usage.sum())\n","\n","    # Ensure all values are non-negative and cap at a reasonable maximum\n","    data = np.clip(data, 0, 1)\n","\n","    return data\n","\n","\n","    \n","epoch_settings = {\n","    'type1': 300,\n","    'PC': 300,\n","    'microwave': 300,\n","    'water':300,\n","    'printer':300,\n","    'coffee':300\n","}\n","\n","\n","def save_to_csv(df, filename):\n","    # Convert the index to the desired string format\n","    df.index = df.index.strftime('%Y-%m-%d %H:%M:%S+04:00')\n","    df.to_csv(filename)\n","\n","\n","\n","if __name__ == \"__main__\":\n","    appliances = {\n","            'type1': ['Fridge-Energy_Filled.csv', 'Kettle-Energy_Filled.csv'],\n","            'PC': ['PC1-Energy_Filled.csv'],\n","            'microwave': ['Microwave-Energy_Filled.csv'],\n","            'printer': ['Printer-Energy_Filled.csv'],\n","            'water':['Water-Energy_Filled.csv'],\n","            'coffee':['Coffee-Energy_Filled.csv']\n","        }\n","    for appliance_type, file_list in appliances.items():\n","        for file_name in file_list:\n","            file_path = f\"/kaggle/input/filled/{file_name}\"\n","            appliance_name = file_name.split('-')[0]\n","            \n","            preprocessed = preprocess_data(file_path, appliance_type)\n","            data = preprocessed['data']\n","            energy_scaler = preprocessed['energy_scaler']\n","            time_scaler = preprocessed['time_scaler']\n","\n","            X = prepare_sequences(data)\n","          \n","\n","            seq_len = 6\n","            n_features = data.shape[1]\n","            hidden_dim = 256\n","            learning_rate = 0.0001\n","\n","            timegan = TimeGAN(seq_len, n_features, hidden_dim, appliance_type)\n","            timegan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n","            \n","         \n","            \n","            epochs = epoch_settings[appliance_type]\n","            \n","            batch_size = 128\n","            steps_per_epoch = len(X) // batch_size\n","\n","            for epoch in range(epochs):\n","                g_losses, d_losses, e_losses, r_losses, p_losses, peak_losses = [], [], [], [], [], []\n","                for step in range(steps_per_epoch):\n","                    idx = np.random.randint(0, len(X), batch_size)\n","                    batch_data = X[idx]\n","                    g_loss, d_loss, e_loss, r_loss, p_loss, peak_loss = timegan.train_step(batch_data)\n","\n","                    if np.isnan([g_loss, d_loss, e_loss, r_loss, p_loss, peak_loss]).any():\n","                        print(f\"NaN detected at epoch {epoch}, step {step}. Skipping this batch.\")\n","                        continue\n","\n","                    g_losses.append(g_loss)\n","                    d_losses.append(d_loss)\n","                    e_losses.append(e_loss)\n","                    r_losses.append(r_loss)\n","                    p_losses.append(p_loss)\n","                    peak_losses.append(peak_loss)\n","\n","                if (epoch + 1) % 50 == 0:\n","                    print(f'Epoch {epoch+1}, G_loss: {np.mean(g_losses):.4f}, D_loss: {np.mean(d_losses):.4f}, '\n","                          f'E_loss: {np.mean(e_losses):.4f}, R_loss: {np.mean(r_losses):.4f}, '\n","                          f'P_loss: {np.mean(p_losses):.4f}, Peak_loss: {np.mean(peak_losses):.4f}')\n","\n","            # Generate synthetic data up to 31-12-2024\n","            end_date = pd.Timestamp('2024-12-31 23:59:59')\n","            original_duration = data.index[-1] - data.index[0]\n","            synthetic_duration = end_date - data.index[-1]\n","            n_samples = max(len(data), int(len(data) * (synthetic_duration / original_duration)))\n","\n","         \n","            random_noise = tf.random.normal([n_samples, seq_len, n_features], dtype=tf.float32)\n","            synthetic_data = timegan.generator(random_noise, training=False).numpy()\n","\n","            synthetic_dates = pd.date_range(start=data.index[-1] + pd.Timedelta(seconds=1), \n","                                            end=end_date, \n","                                            periods=n_samples)\n","\n","            synthetic_df = pd.DataFrame(synthetic_data[:, 0, :], \n","                                        columns=data.columns,\n","                                        index=synthetic_dates)\n","\n","            # Inverse transform the data\n","            synthetic_df['kWh.mean_value'] = energy_scaler.inverse_transform(synthetic_df[['kWh.mean_value']])\n","            synthetic_df[['day_of_week', 'hour', 'is_weekend', 'month', 'season']] = time_scaler.inverse_transform(synthetic_df[['day_of_week', 'hour', 'is_weekend', 'month', 'season']])\n","\n","            # Appliance-specific post-processing\n","            original_data = pd.DataFrame(energy_scaler.inverse_transform(data[['kWh.mean_value']]), \n","                                         columns=['kWh.mean_value'], \n","                                         index=data.index)\n","            synthetic_df = post_process_appliance(synthetic_df, appliance_type, energy_scaler, original_data)\n","\n","            # Combine original and synthetic data\n","            combined_df = pd.concat([original_data, synthetic_df[['kWh.mean_value']]])\n","\n","            # Plot combined data\n","            plt.figure(figsize=(20, 8))\n","            plt.plot(combined_df.index, combined_df['kWh.mean_value'], label='Combined Data', alpha=0.7)\n","            plt.axvline(x=data.index[-1], color='r', linestyle='--', label='Original Data End')\n","            plt.title(f'Combined Original and Synthetic Energy Consumption Data - {appliance_name}')\n","            plt.xlabel('Date')\n","            plt.ylabel('kWh')\n","            plt.legend()\n","            plt.tight_layout()\n","            plt.savefig(f'{appliance_name}_combined_data.png')\n","            \n","            \n","       \n","            \n","            \n","            plt.show()\n","\n","            # In the main loop where you save the combined data:\n","            output_file = f'{appliance_name}_augmented_data.csv'\n","            save_to_csv(combined_df, output_file)\n","            print(f\"Combined data saved to {output_file}\")\n","\n","            # Original vs. Synthetic Plot\n","            plt.figure(figsize=(14, 6))\n","            plt.plot(original_data.index, original_data['kWh.mean_value'], label='Original Data')\n","            plt.plot(synthetic_df.index[:len(original_data)], synthetic_df['kWh.mean_value'][:len(original_data)], label='Synthetic Data', linestyle='--')\n","            plt.xlabel('Date')\n","            plt.ylabel('kWh')\n","            plt.title(f'Original vs. Synthetic Energy Consumption - {appliance_name}')\n","            plt.legend()\n","            plt.savefig(f'{appliance_name}_original_vs_synthetic.png')\n","            plt.show()\n","\n","            print(f\"Completed processing for {appliance_name}\\n\")\n","\n","    print(\"All appliances processed successfully.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5546410,"sourceId":9177250,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
